{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text sequencing with DeepSeries\n",
    "\n",
    "### In this example we show how to train DeepSeries on a text corpus and let the network predict future sentences. We use Alice in Wonderland and train on character series, so that in the end we can input a leading sentence and get the continuation from the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "from deepseries import deepseries, preprocessing\n",
    "import encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Alice in Wonderland\n",
    "with open('alice_in_wonderland_FORMATTED.txt', 'rb') as f:\n",
    "    text_corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feed into encoder class. The code for this is in a separate file.\n",
    "# It performs transformations between text characters and one-hot binary\n",
    "# encoded labels.\n",
    "Enc = encoder.Encoder(text_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split string into sequences, like 15 characters \n",
    "X,y = preprocessing.Slicer().fit(Enc.onehot_corpus, sequence_length=15,kind='seq2seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test/train split. Test is the trailing 20% of the corpus\n",
    "split_index = int(0.8*X.shape[0])\n",
    "Xtrain, Xtest, ytrain, ytest =  \\\n",
    "X[:split_index], X[split_index:], y[:split_index], y[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize DeepSeries sequencer with classification learning.\n",
    "# Increasing Nnodes or Nlayers will increase the size (and complexity)\n",
    "# of the network\n",
    "ds = deepseries.Sequencer(Xtrain, ytrain, Xtest, ytest,\n",
    "    Nnodes=150, \n",
    "    Nlayers=2,\n",
    "    input_keep_prob=.9,\n",
    "    output_keep_prob=.9,\n",
    "    optimizer_type='AdamOptimizer', \n",
    "    optimizer_kwargs={'learning_rate':0.001},\n",
    "    cell_kwargs = {'use_peepholes':False, \n",
    "                   'forget_bias':1,\n",
    "                   'state_is_tuple':True,\n",
    "                   'activation':deepseries.tf.nn.relu},\n",
    "    batch_size=60,\n",
    "    cell_type='GRUCell',\n",
    "    tracking_step=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch cost: 2944.771, test cost: 1330471.275 \n",
      "train batch cost: 1985.973, test cost: 910561.092 \n",
      "train batch cost: 1785.021, test cost: 819863.075 \n",
      "train batch cost: 1702.215, test cost: 787322.093 \n",
      "train batch cost: 1599.467, test cost: 759941.718 \n",
      "train batch cost: 1593.731, test cost: 747247.351 \n",
      "train batch cost: 1624.323, test cost: 734177.895 \n",
      "train batch cost: 1539.692, test cost: 729728.033 \n",
      "train batch cost: 1435.394, test cost: 717685.258 \n",
      "train batch cost: 1560.239, test cost: 713081.357 \n",
      "train batch cost: 1496.891, test cost: 712338.882 \n",
      "train batch cost: 1473.880, test cost: 709090.291 \n",
      "train batch cost: 1423.819, test cost: 704028.349 \n",
      "train batch cost: 1538.557, test cost: 699864.659 \n",
      "train batch cost: 1456.718, test cost: 696315.014 \n",
      "train batch cost: 1414.644, test cost: 692815.532 \n",
      "train batch cost: 1392.590, test cost: 689580.917 \n",
      "train batch cost: 1412.664, test cost: 689562.559 \n",
      "train batch cost: 1477.363, test cost: 687777.463 \n",
      "train batch cost: 1447.342, test cost: 687588.822 \n"
     ]
    }
   ],
   "source": [
    "# Do some light fitting. If you are running the code yourself, \n",
    "# start with one epoch to test your computing speed.\n",
    "ds.fit(n_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ot thrown out t', 'he said the moc'),\n",
       " ('out to sea so t', 'he same to hers'),\n",
       " (' so they had to', ' say would to h'),\n",
       " ('ad to fall a lo', 'ng the morse sa'),\n",
       " (' a long way so ', 'she was to hers'),\n",
       " ('y so they got t', 'he door and the'),\n",
       " ('got their tails', ' in t at the ro'),\n",
       " ('tails fast in t', 'he door and too'),\n",
       " (' in their mouth', ' and the rabbit'),\n",
       " ('mouths so they ', 'would the rabbi')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what the network outputs if we feed in\n",
    "# chunks from the test set:\n",
    "test_sample = Xtest[::10][:10]\n",
    "input_sentences = Enc.onehot2chars(test_sample)\n",
    "output_sentences = Enc.onehot2chars(ds.unroll(test_sample, n_output=15))\n",
    "zip(input_sentences, output_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the reased a little said the r'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also use this wrapper method to input \n",
    "# any sentence you can think of, of any length. \n",
    "# You can also set any number of output characters:\n",
    "Enc.talk(ds, 'alice was drinking tea and ', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network trained only on character sequence, but it was able to learn the structure of words and how to put them together, handling blank spaces quite well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
